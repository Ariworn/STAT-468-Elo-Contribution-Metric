[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian Modelling of Elo Contribution",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "00-intro.html",
    "href": "00-intro.html",
    "title": "1  Abstract",
    "section": "",
    "text": "The Motivating Idea\nElo generally represents the relative strength of a player or team compared to others by considering their past results. We are looking at elo for soccer clubs in the 2023-24 and 2024-25 premier league seasons.\n\\(\\Delta\\) Elo is defined as: the difference in elo before a match was played and the elo after a match was played for a premier league team. It represents the loss or gain of relative strength.\nUsing \\(\\Delta\\) Elo as our dependent variable, the idea is to use individual player ratings taken from a website that publishes these ratings, to model a player’s contribution to the delta elo in that match. This is similar to making an adjusted plus-minus model, and the current framework makes use of an idea from (Matano et al.) in their paper titled “Augmenting Adjusted Plus-Minus in Soccer with FIFA Ratings.” In it they reframe an Adjusted Plus Minus (APM) model framework into a Bayesian model by incorporating FIFA ratings into the prior distribution.\nSince Goal difference and delta_elo are similar, the idea is to use similar modelling. What we attempt to do is use a Bayesian Hierarchical Model to explain how much a players relative skill has an impact on their team’s \\(\\Delta\\) Elo.\nMore details about the model are found here: Model",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Abstract</span>"
    ]
  },
  {
    "objectID": "01-Import.html",
    "href": "01-Import.html",
    "title": "2  Import",
    "section": "",
    "text": "For the project here are all the list of packages that were installed and exist on the requirements.txt file.\n\n\nCode\n#Libraries used in the project\n#Basic libraries\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\nfrom io import StringIO #yes\n#Data visualization libraries\nfrom lets_plot import *\nLetsPlot.setup_html()\nfrom great_tables import GT\nimport arviz as az\n#Web scraping libraries\nimport selenium as se\nfrom curl_cffi import requests\nfrom bs4 import BeautifulSoup\nimport time\n#For database connection and manipulation \nimport duckdb\n#For Hierarchical modeling\nimport pymc as pm\nimport xarray as xr\n#for pyshiny\nimport shiny\nimport requests as req\nimport vetiver\nimport pins\nimport logging \nimport json\nimport plotly.express as px\n\n\n\n            \n            \n            \n\n\nSome considerations regarding the packages: 1. I needed selenium to run an automated browser to bypass certain obstacles in scraping player ratings for each match. A lot of the code is heavily adapted from chat gpt (https://chatgpt.com/share/689d0e3d-8f90-800c-9166-fe3dc3e3454f)\n\nInitially I was webscraping sofascore ratings but wasted so much time to no avail but learnt about a really cool package called curl_cffi from reddit that helps you impersonate your browser by using a header-agent. I continued using the same package. https://github.com/lexiforest/curl_cffi\n\nDatasets that were extracted/scraped and used with relevant considerations:\n\nFinding all the match fixtures that happened in the 2023/24 and 2024/25 seasons using this website: https://www.football-data.co.uk/englandm.php which has csv files for all the matches with home and away teams and the score line at the end of the match.\nWebscraping footballcritic to extract unique match_id in a particular season for each premier league team. For this the season_id was unique and was the ‘slug’ which has the season name and team names. Here is an example url https://www.footballcritic.com/premier-league/season-2024-2025/matches/2/72764\nGenerally the idea was to use the api to find club elo information from this website http://clubelo.com/. However, I encountered many problems with the final dataset clubelo also included premier league and champions league (and other european competition fixtures) and did not have a consistent daily elo dataset that could be used.\nInstead I created my own elo model using the methodology listed here: https://www.footballdatabase.com/methodology. Moreover, instead of starting at a baseline 1500 I used the preseason elo ratings from clubelo’s website that I manually extracted for each premier league club.\n\nHere are examples of how all datasets actually look like before tidying and transformation:\n\nFootball-data\n\n\n\nCode\nseason_23_24 = pd.read_csv('data/2023-24.csv')\nseason_24_25 = pd.read_csv('data/2024-25.csv')\nframes = [season_23_24, season_24_25]\nfull = pd.concat(frames)\nGT(full.head(5))\n\n\n\n\n\n\n\n\nDiv\nDate\nTime\nHomeTeam\nAwayTeam\nFTHG\nFTAG\nFTR\nHTHG\nHTAG\nHTR\nReferee\nHS\nAS\nHST\nAST\nHF\nAF\nHC\nAC\nHY\nAY\nHR\nAR\nB365H\nB365D\nB365A\nBWH\nBWD\nBWA\nIWH\nIWD\nIWA\nPSH\nPSD\nPSA\nWHH\nWHD\nWHA\nVCH\nVCD\nVCA\nMaxH\nMaxD\nMaxA\nAvgH\nAvgD\nAvgA\nB365&gt;2.5\nB365&lt;2.5\nP&gt;2.5\nP&lt;2.5\nMax&gt;2.5\nMax&lt;2.5\nAvg&gt;2.5\nAvg&lt;2.5\nAHh\nB365AHH\nB365AHA\nPAHH\nPAHA\nMaxAHH\nMaxAHA\nAvgAHH\nAvgAHA\nB365CH\nB365CD\nB365CA\nBWCH\nBWCD\nBWCA\nIWCH\nIWCD\nIWCA\nPSCH\nPSCD\nPSCA\nWHCH\nWHCD\nWHCA\nVCCH\nVCCD\nVCCA\nMaxCH\nMaxCD\nMaxCA\nAvgCH\nAvgCD\nAvgCA\nB365C&gt;2.5\nB365C&lt;2.5\nPC&gt;2.5\nPC&lt;2.5\nMaxC&gt;2.5\nMaxC&lt;2.5\nAvgC&gt;2.5\nAvgC&lt;2.5\nAHCh\nB365CAHH\nB365CAHA\nPCAHH\nPCAHA\nMaxCAHH\nMaxCAHA\nAvgCAHH\nAvgCAHA\nBFH\nBFD\nBFA\n1XBH\n1XBD\n1XBA\nBFEH\nBFED\nBFEA\nBFE&gt;2.5\nBFE&lt;2.5\nBFEAHH\nBFEAHA\nBFCH\nBFCD\nBFCA\n1XBCH\n1XBCD\n1XBCA\nBFECH\nBFECD\nBFECA\nBFEC&gt;2.5\nBFEC&lt;2.5\nBFECAHH\nBFECAHA\n\n\n\n\nE0\n11/08/2023\n20:00\nBurnley\nMan City\n0\n3\nA\n0\n2\nA\nC Pawson\n6\n17\n1\n8\n11\n8\n6\n5\n0\n0\n1\n0\n8.0\n5.5\n1.33\n8.75\n5.25\n1.34\n8.0\n5.5\n1.35\n8.58\n5.51\n1.37\n8.0\n5.0\n1.25\n9.5\n5.25\n1.33\n9.5\n5.68\n1.39\n9.02\n5.35\n1.35\n1.67\n2.2\n1.68\n2.29\n1.71\n2.4\n1.65\n2.27\n1.5\n1.86\n2.07\n1.86\n2.07\n1.93\n2.09\n1.85\n2.03\n9.0\n5.25\n1.33\n8.75\n5.25\n1.33\n8.5\n5.25\n1.35\n9.62\n5.81\n1.33\n7.5\n4.6\n1.29\n10.5\n5.25\n1.3\n10.5\n5.81\n1.36\n9.27\n5.45\n1.33\n1.67\n2.2\n1.65\n2.35\n1.73\n2.45\n1.64\n2.28\n1.5\n1.95\n1.98\n1.95\n1.97\n\n\n1.92\n1.95\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nE0\n12/08/2023\n12:30\nArsenal\nNott'm Forest\n2\n1\nH\n2\n0\nH\nM Oliver\n15\n6\n7\n2\n12\n12\n8\n3\n2\n2\n0\n0\n1.18\n7.0\n15.0\n1.17\n7.5\n15.5\n1.2\n7.25\n14.0\n1.18\n7.86\n15.67\n1.12\n6.5\n12.0\n1.14\n7.5\n17.0\n1.21\n8.5\n17.5\n1.18\n7.64\n15.67\n1.44\n2.75\n1.42\n2.93\n1.45\n2.98\n1.42\n2.85\n-2.0\n1.88\n2.02\n1.88\n2.01\n1.91\n2.06\n1.87\n1.99\n1.18\n7.0\n15.0\n1.18\n7.0\n14.5\n1.2\n7.0\n14.0\n1.19\n8.0\n16.0\n1.12\n6.5\n12.0\n1.22\n7.0\n13.0\n1.22\n8.4\n19.0\n1.19\n7.43\n15.98\n1.5\n2.63\n1.49\n2.65\n1.52\n2.79\n1.49\n2.63\n-2.0\n1.95\n1.98\n1.93\n1.97\n2.01\n2.09\n1.95\n1.92\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nE0\n12/08/2023\n15:00\nBournemouth\nWest Ham\n1\n1\nD\n0\n0\nD\nP Bankes\n14\n16\n5\n3\n9\n14\n10\n4\n1\n4\n0\n0\n2.7\n3.4\n2.55\n2.65\n3.4\n2.55\n2.7\n3.45\n2.6\n2.7\n3.47\n2.71\n2.62\n3.2\n2.3\n2.63\n3.3\n2.63\n2.8\n3.62\n2.75\n2.69\n3.44\n2.64\n1.9\n2.0\n1.9\n1.99\n1.95\n2.03\n1.88\n1.94\n0.0\n1.95\n1.95\n1.95\n1.95\n1.98\n1.99\n1.94\n1.92\n2.63\n3.5\n2.6\n2.65\n3.5\n2.5\n2.6\n3.5\n2.6\n2.75\n3.6\n2.63\n2.5\n3.2\n2.45\n2.63\n3.5\n2.6\n2.88\n3.67\n2.7\n2.7\n3.53\n2.59\n1.73\n2.1\n1.76\n2.18\n1.83\n2.23\n1.74\n2.12\n0.0\n2.02\n1.91\n2.01\n1.92\n2.06\n1.96\n1.96\n1.91\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nE0\n12/08/2023\n15:00\nBrighton\nLuton\n4\n1\nH\n1\n0\nH\nD Coote\n27\n9\n12\n3\n11\n12\n6\n7\n2\n2\n0\n0\n1.33\n5.5\n9.0\n1.32\n5.5\n9.0\n1.35\n5.25\n8.5\n1.33\n5.65\n9.61\n1.25\n4.6\n8.5\n1.29\n5.25\n10.0\n1.36\n6.0\n10.5\n1.33\n5.52\n9.61\n1.62\n2.3\n1.62\n2.4\n1.65\n2.45\n1.61\n2.34\n-1.5\n1.95\n1.95\n1.95\n1.95\n1.98\n2.0\n1.93\n1.93\n1.25\n6.5\n11.0\n1.26\n6.0\n11.0\n1.3\n5.5\n9.5\n1.27\n6.36\n11.36\n1.22\n5.5\n9.0\n1.25\n5.75\n13.0\n1.34\n6.59\n13.0\n1.28\n5.99\n10.91\n1.53\n2.5\n1.56\n2.54\n1.62\n2.66\n1.55\n2.48\n-1.75\n2.01\n1.92\n2.0\n1.91\n2.14\n1.93\n2.0\n1.86\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nE0\n12/08/2023\n15:00\nEverton\nFulham\n0\n1\nA\n0\n0\nD\nS Attwell\n19\n9\n9\n2\n12\n6\n10\n4\n0\n2\n0\n0\n2.2\n3.4\n3.3\n2.2\n3.4\n3.25\n2.25\n3.4\n3.25\n2.27\n3.45\n3.35\n2.1\n3.2\n3.0\n2.2\n3.3\n3.25\n2.3\n3.57\n3.45\n2.24\n3.43\n3.3\n2.01\n1.89\n2.0\n1.89\n2.04\n1.92\n1.97\n1.86\n-0.25\n1.93\n1.97\n1.95\n1.95\n1.97\n2.0\n1.92\n1.93\n2.3\n3.2\n3.2\n2.35\n3.2\n3.1\n2.35\n3.25\n3.15\n2.39\n3.32\n3.3\n2.1\n3.2\n3.0\n2.38\n3.2\n3.2\n2.42\n3.42\n3.56\n2.32\n3.28\n3.27\n2.1\n1.73\n2.23\n1.72\n2.26\n1.81\n2.17\n1.71\n-0.25\n2.06\n1.87\n2.04\n1.88\n2.08\n1.99\n1.98\n1.88\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMatch_id from footballcritic\n\n\n\nCode\nMatch_info = pd.read_csv('data/team_match_ids_with_dates_and_comps.csv')\nGT(Match_info.head(5))\n\n\n\n\n\n\n\n\nteam_name\nteam_id\nseason\nseason_id\nmatch_id\ndate\ncompetition\n\n\n\n\nWolverhampton Wanderers FC\n459\nseason-2023-2024\n68731\n3009117\nAug 14, 2023\nEPL\n\n\nWolverhampton Wanderers FC\n459\nseason-2023-2024\n68731\n3009127\nAug 19, 2023\nEPL\n\n\nWolverhampton Wanderers FC\n459\nseason-2023-2024\n68731\n3009134\nAug 26, 2023\nEPL\n\n\nWolverhampton Wanderers FC\n459\nseason-2023-2024\n68731\n3131390\nAug 29, 2023\nFLC\n\n\nWolverhampton Wanderers FC\n459\nseason-2023-2024\n68731\n3009143\nSep 3, 2023\nEPL\n\n\n\n\n\n\n\n\n\nElo across seasons and matches\n\n\n\nCode\nelo = pd.read_csv('data/Elo_Table_Final.csv')\nGT(elo.head(5))\n\n\n\n\n\n\n\n\nseason_id\nmatch_id\ndate\nhome_team_id\naway_team_id\nold_home_elo\nold_away_elo\nnew_home_elo\nnew_away_elo\ndelta_home\ndelta_away\n\n\n\n\n68731\n3009108\n2023-08-11\n492\n464\n1726.4185791\n2080.86743164\n1716.5619319002355\n2090.7240788397644\n-9.856647199764438\n9.856647199764437\n\n\n68731\n3009114\n2023-08-12\n466\n461\n1887.082153\n1816.943237\n1902.4390321331364\n1801.586357866864\n15.356879133136228\n-15.356879133136228\n\n\n68731\n3009113\n2023-08-12\n480\n508\n1644.08129883\n1757.40673828\n1629.6563223701708\n1771.831714739829\n-14.424976459829114\n14.424976459829114\n\n\n68731\n3009112\n2023-08-12\n473\n460\n1699.523682\n1745.379028\n1682.204811369241\n1762.697898630759\n-17.31887063075889\n17.31887063075889\n\n\n68731\n3009111\n2023-08-12\n510\n502\n1832.391968\n1602.344727\n1839.2232295331976\n1595.513465466802\n6.8312615331978925\n-6.8312615331978925\n\n\n\n\n\n\n\n\n\nPlayer ratings webscraped from footballcritic\n\n\n\nCode\nall_ratings = pd.read_csv('data/player_ratings.csv')\nGT(all_ratings.head(5))\n\n\n\n\n\n\n\n\nMatch ID\nTeam ID\nPlayer\nRating\n\n\n\n\n3009117\n475\nA. Onana\n8.2\n\n\n3009117\n475\nA. W-Bissaka\n7.6\n\n\n3009117\n475\nR. Varane\n7.4\n\n\n3009117\n475\nL. Martínez\n6.1\n\n\n3009117\n475\nL. Shaw\n7.2",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Import</span>"
    ]
  },
  {
    "objectID": "02-Tidy-Transform.html",
    "href": "02-Tidy-Transform.html",
    "title": "3  Tidy and Transform",
    "section": "",
    "text": "Set up\n\n\nCode\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\nfrom io import StringIO #yes\n#Data visualization libraries\nfrom lets_plot import *\nLetsPlot.setup_html()\nfrom great_tables import GT\nimport arviz as az\n#Web scraping libraries\nimport selenium as se\nfrom curl_cffi import requests\nfrom bs4 import BeautifulSoup\nimport time\n#For database connection and manipulation \nimport duckdb\n#For Hierarchical modeling\nimport pymc as pm\nimport xarray as xr\n#for pyshiny\nimport shiny\nimport requests as req\nimport vetiver\nimport pins\nimport logging \nimport json\nimport plotly.express as px\n\n\n\n            \n            \n            \n\n\nIt makes sense to do this one, as all the processes flow into one and were initially taken from a juptyr notebook.\nCreating an Elo dataset I’m trying to create a dataset that would allow me to run the code I wrote to create prematch and postmatch elo for each team on the home and away side of a match. Here are the steps:\nChanging some of the column names for readability and concatenating the data from football-data\n\n\nCode\nseason_23_24 = pd.read_csv('data/2023-24.csv')\nseason_24_25 = pd.read_csv('data/2024-25.csv')\nframes = [season_23_24, season_24_25]\nfull = pd.concat(frames)\nfull.rename(columns={'FTHG': 'Home_Goals'}, inplace=True)\nfull.rename(columns={'FTAG': 'Away_Goals'}, inplace=True)\nfull.rename(columns={'FTR': 'Result'}, inplace=True)\nteam_ids = pd.read_csv('data/team_ids.csv')\nGT(team_ids.head(5))\n\n\n\n\n\n\n\n\nSeason\nteam_id\nslug\n\n\n\n\nseason-2023-2024\n459\nWolverhampton Wanderers FC\n\n\nseason-2024-2025\n459\nWolverhampton Wanderers FC\n\n\nseason-2023-2024\n460\nFulham FC\n\n\nseason-2024-2025\n460\nFulham FC\n\n\nseason-2023-2024\n461\nAston Villa FC\n\n\n\n\n\n\n\n\nThe webscraper for match_ids gives us data that looks like this\n\n\nCode\nMatch_info = pd.read_csv('data/team_match_ids_with_dates_and_comps.csv')\nGT(Match_info.head(5))\n\n\n\n\n\n\n\n\nteam_name\nteam_id\nseason\nseason_id\nmatch_id\ndate\ncompetition\n\n\n\n\nWolverhampton Wanderers FC\n459\nseason-2023-2024\n68731\n3009117\nAug 14, 2023\nEPL\n\n\nWolverhampton Wanderers FC\n459\nseason-2023-2024\n68731\n3009127\nAug 19, 2023\nEPL\n\n\nWolverhampton Wanderers FC\n459\nseason-2023-2024\n68731\n3009134\nAug 26, 2023\nEPL\n\n\nWolverhampton Wanderers FC\n459\nseason-2023-2024\n68731\n3131390\nAug 29, 2023\nFLC\n\n\nWolverhampton Wanderers FC\n459\nseason-2023-2024\n68731\n3009143\nSep 3, 2023\nEPL\n\n\n\n\n\n\n\n\nAfter this I change the date into datetime, filter for premier league matches and create a mapping between the names in football-data.co.uk and the ones used in footballcritc.\n\n\nCode\nMatch_info = pd.read_csv('data/team_match_ids_with_dates_and_comps.csv')\nMatch_info.drop(Match_info[Match_info['competition'] != 'EPL'].index, inplace = True)\nMatch_info['date'] = pd.to_datetime(Match_info['date'], format='%b %d, %Y')\nMatch_info\nmyelo = full.copy()#from football-data\nmyelo = myelo[['Date', 'HomeTeam', 'AwayTeam', 'Home_Goals', 'Away_Goals']]\nmyelo['Date'] = pd.to_datetime(myelo['Date'], format= '%d/%m/%Y')\nname_mapping = {\n    \"Man United\": \"Manchester United FC\",\n    \"Wolves\": \"Wolverhampton Wanderers FC\",\n    \"Everton\": \"Everton FC\",\n    \"Chelsea\": \"Chelsea FC\",\n    \"Man City\": \"Manchester City FC\",\n    \"Norwich\": \"Norwich City FC\",\n    \"Sheffield United\": \"Sheffield United FC\",\n    \"Brighton\": \"Brighton & Hove Albion FC\",\n    \"Aston Villa\": \"Aston Villa FC\",\n    \"Leeds\": \"Leeds United FC\",\n    \"Fulham\": \"Fulham FC\",\n    \"Brentford\": \"Brentford FC\",\n    \"Southampton\": \"Southampton FC\",\n    \"Burnley\": \"Burnley FC\",\n    \"Newcastle\": \"Newcastle United FC\",\n    \"Arsenal\": \"Arsenal FC\",\n    \"West Ham\": \"West Ham United FC\",\n    \"Nott'm Forest\": \"Nottingham Forest FC\",\n    \"Luton\": \"Luton Town FC\",\n    \"Ipswich\": \"Ipswich Town FC\",\n    \"Tottenham\": \"Tottenham Hotspur FC\",\n    \"Leicester\": \"Leicester City FC\",\n    \"Bournemouth\": \"AFC Bournemouth\",\n    \"Liverpool\": \"Liverpool FC\",\n    \"Crystal Palace\": \"Crystal Palace FC\"\n}\nmyelo = myelo.replace(name_mapping)\nGT(myelo.head(5))\n\n\n\n\n\n\n\n\nDate\nHomeTeam\nAwayTeam\nHome_Goals\nAway_Goals\n\n\n\n\n2023-08-11\nBurnley FC\nManchester City FC\n0\n3\n\n\n2023-08-12\nArsenal FC\nNottingham Forest FC\n2\n1\n\n\n2023-08-12\nAFC Bournemouth\nWest Ham United FC\n1\n1\n\n\n2023-08-12\nBrighton & Hove Albion FC\nLuton Town FC\n4\n1\n\n\n2023-08-12\nEverton FC\nFulham FC\n0\n1\n\n\n\n\n\n\n\n\nTo create a dataframe that has home and away teams in separate columns I match the team_names with the home or away team id with an inner join. The final output looks like this:\n\n\nCode\nconn = duckdb.connect()\nconn.register('ids', Match_info) #football critic data\nconn.register('myelo', myelo) # from football-uk.co\nquery = \"\"\"\nSELECT \n    a.season_id, a.match_id,\n    b.*\nFROM ids a\nINNER JOIN myelo b\nON a.date = b.Date\nAND (a.team_name = b.HomeTeam OR a.team_name = b.AwayTeam)\nORDER BY b.Date\n\"\"\"\nresult = conn.execute(query).df()\nresult.to_csv('data/Elo_Table_Pre.csv', index=False)  # Changed from df to result\nconn.close()\nresult = result.drop_duplicates()\nresult = result.replace('Brighton & Hove Albion FC', 'Brighton Hove Albion FC')\n#Elo Creater\nteam_id_map = Match_info.copy()\nteam_id_map = team_id_map.loc[(team_id_map['season'] == 'season-2024-2025') | (team_id_map['season'] == 'season-2023-2024')]#to account for promoted teams\nteam_id_map['team_name'] = team_id_map['team_name'].replace('Brighton & Hove Albion FC', 'Brighton Hove Albion FC')\nteam_id_map = team_id_map.set_index('team_name')['team_id'].to_dict()\nresult['home_team_id'] = result['HomeTeam'].map(team_id_map)\nresult['away_team_id'] = result['AwayTeam'].map(team_id_map)\nmatches = result[['season_id','match_id', 'Date', 'home_team_id', 'away_team_id', 'Home_Goals', 'Away_Goals']]\nmatches\nGT(matches.head(5))\n\n\n\n\n\n\n\n\nseason_id\nmatch_id\nDate\nhome_team_id\naway_team_id\nHome_Goals\nAway_Goals\n\n\n\n\n68731\n3009108\n2023-08-11\n492\n464\n0\n3\n\n\n68731\n3009114\n2023-08-12\n466\n461\n5\n1\n\n\n68731\n3009113\n2023-08-12\n480\n508\n0\n1\n\n\n68731\n3009112\n2023-08-12\n473\n460\n0\n1\n\n\n68731\n3009111\n2023-08-12\n510\n502\n4\n1\n\n\n\n\n\n\n\n\nNext I create a dictionary for team_ids with the starting elo I took from clubelo’s website such that 475 = 1877 for instance. Then I generate the elo for each match pre and post result. Using this we are able to get consistent positive and negative (zero-sum) \\(\\Delta\\) Elo’s for the winning or losing team (which was not possible with clubelo api).\n\n\nCode\nstarting_elo = {\"Manchester United FC\":1877.080444,\n    \"Wolverhampton Wanderers FC\":1715.651001,\n    \"Everton FC\":1699.523682,\n    \"Chelsea FC\":1788.11755371,\n    \"Manchester City FC\": 2080.86743164,\n    \"Sheffield United FC\":1644.08129883,\n    \"Brighton Hove Albion FC\":1832.391968,\n    \"Aston Villa FC\":1816.943237,\n    \"Fulham FC\": 1745.379028,\n    \"Brentford FC\":1824.3449707,\n    \"Southampton FC\":1599.60339355,\n    \"Burnley FC\":1726.4185791,\n    \"Newcastle United FC\":1887.082153,\n    \"Arsenal FC\":1921.475342,\n    \"West Ham United FC\": 1777.576782,\n    \"Nottingham Forest FC\": 1671.890747,\n    \"Luton Town FC\":1602.344727,\n    \"Ipswich Town FC\":1568.32556152,\n    \"Tottenham Hotspur FC\":1815.88818359,\n    \"Leicester City FC\":1643.66943359,\n    \"AFC Bournemouth\":1660.65661621,\n    \"Liverpool FC\":1943.996216,\n    \"Crystal Palace FC\":1757.40673828}\nstarting_elo # unit test to check if there's 760 and also if delta elo is appropriate\nteam_id_map\nid_to_name = {tid: name for name, tid in team_id_map.items()}\nid_to_elo = {tid: starting_elo[name] for tid, name in id_to_name.items()}\n\nK = 30\nhome_adv_pts = 100  \nimport pandas as pd\n\ndef goal_diff_factor(goal_diff):\n    if goal_diff &lt;= 1:\n        return 1.0\n    elif goal_diff == 2:\n        return 1.5\n    else:\n        return (11 + goal_diff) / 8.0\n\ndef expected_prob(r_a, r_b, home_adv_pts=100.0):\n    return 1.0 / (1.0 + 10 ** ((r_b - (r_a + home_adv_pts)) / 400.0))\n\ndef update_elo(matches_df, id_to_elo, K=30, home_adv_pts=100.0):\n    elo_history = []\n\n    for _, row in matches_df.sort_values(\"Date\").iterrows():\n        season_id = row[\"season_id\"]\n        match_id = row[\"match_id\"]\n        home_id = row[\"home_team_id\"]\n        away_id = row[\"away_team_id\"]\n        home_goals = row[\"Home_Goals\"]\n        away_goals = row[\"Away_Goals\"]\n\n        # Get current ratings\n        r_home = id_to_elo[home_id]\n        r_away = id_to_elo[away_id]\n\n        # Match result (W)\n        if home_goals &gt; away_goals:\n            w_home, w_away = 1.0, 0.0\n        elif home_goals &lt; away_goals:\n            w_home, w_away = 0.0, 1.0\n        else:\n            w_home, w_away = 0.5, 0.5\n\n        # Expected result (We)\n        we_home = expected_prob(r_home, r_away, home_adv_pts)\n        we_away = 1 - we_home\n\n        # Goal difference factor (G)\n        g = goal_diff_factor(abs(home_goals - away_goals))\n\n        # New ratings\n        new_r_home = r_home + K * g * (w_home - we_home)\n        new_r_away = r_away + K * g * (w_away - we_away)\n\n        delta_home = K * g * (w_home - we_home)\n        delta_away = K * g * (w_away - we_away) \n        # Store history with match_id\n        elo_history.append({\n            \"season_id\":season_id,\n            \"match_id\": match_id,\n            \"date\": row[\"Date\"],\n            \"home_team_id\": home_id,\n            \"away_team_id\": away_id,\n            \"old_home_elo\": r_home,\n            \"old_away_elo\": r_away,\n            \"new_home_elo\": new_r_home,\n            \"new_away_elo\": new_r_away,\n            \"delta_home\": delta_home,\n            \"delta_away\": delta_away\n        })\n\n        # Update ratings for next match\n        id_to_elo[home_id] = new_r_home\n        id_to_elo[away_id] = new_r_away\n\n    return pd.DataFrame(elo_history)\n\nfinale = update_elo(matches,id_to_elo, K=30, home_adv_pts=100.0)\nx = finale.to_csv('data/Elo_Table_Final.csv', index=False) \nGT(finale.head(5))\n\n\n\n\n\n\n\n\nseason_id\nmatch_id\ndate\nhome_team_id\naway_team_id\nold_home_elo\nold_away_elo\nnew_home_elo\nnew_away_elo\ndelta_home\ndelta_away\n\n\n\n\n68731\n3009108\n2023-08-11\n492\n464\n1726.4185791\n2080.86743164\n1716.5619319002355\n2090.7240788397644\n-9.856647199764438\n9.856647199764437\n\n\n68731\n3009114\n2023-08-12\n466\n461\n1887.082153\n1816.943237\n1902.4390321331364\n1801.5863578668636\n15.356879133136227\n-15.356879133136227\n\n\n68731\n3009113\n2023-08-12\n480\n508\n1644.08129883\n1757.40673828\n1629.6563223701708\n1771.831714739829\n-14.424976459829114\n14.424976459829114\n\n\n68731\n3009112\n2023-08-12\n473\n460\n1699.523682\n1745.379028\n1682.204811369241\n1762.697898630759\n-17.31887063075889\n17.31887063075889\n\n\n68731\n3009111\n2023-08-12\n510\n502\n1832.391968\n1602.344727\n1839.2232295331978\n1595.513465466802\n6.8312615331978925\n-6.8312615331978925\n\n\n\n\n\n\n\n\nCreating the final dataset for modelling\nThis requires combining the player_ratings dataset created by the webscraping script that takes hours to run and doing relevant tidying and transforming. There’s two big transformations that occur now. 1. There are missing values for certain player ratings in some matches. This leads to the firs transformation; if a player is missing values then replace those values with their overall average, and if even then (and there were) missing values then replace them by their team’s average. 2. The second big transformation comes with changing the signs of the rating from +8.7 to -8.7 for instance if the player was on the away side. This was done primarily for the model to understand our X\\(\\beta\\) and assign negative and positive \\(\\Delta\\) as belonging to the away or home side respectively. This was taken directly from the augmented adjusted plus-minus paper where they do the same transformation.\nHere is how the dataset looks like after those transformations\n\n\nCode\nall_ratings = pd.read_csv('data/player_ratings.csv')\nall_ratings = all_ratings.rename(columns={'Match ID': 'match_id', 'Team ID': 'team_id'})\nconn = duckdb.connect()\nconn.register('finale', finale)\nconn.register('ratings', all_ratings)\nquery = \"\"\"\nSELECT \na.season_id,\na.Date,\na.match_id,\na.home_team_id,\na.away_team_id,\na.old_home_elo,\na.old_away_elo,\na.new_home_elo,\na.new_away_elo,\na.delta_home,\na.delta_away,\nb.team_id as player_team_id,\nb.Player,\nb.Rating as player_rating,\nFROM finale a\nINNER JOIN ratings b\nON a.match_id = b.match_id\n\"\"\"\ndf = conn.execute(query).df()\nconn.close()\n\ndf['is_home'] = df['player_team_id'] == df['home_team_id']\nplayer_avg_rating = df.groupby(\"Player\")[\"player_rating\"].mean()\nteam_avg_rating = df.groupby(\"player_team_id\")[\"player_rating\"].mean()\ndf[\"player_rating\"] = df.apply(lambda row: player_avg_rating[row[\"Player\"]] if pd.isna(row[\"player_rating\"]) else row[\"player_rating\"], axis=1) # their average rating\ndf[\"player_rating\"] = df.apply(lambda row: team_avg_rating[row[\"player_team_id\"]] if pd.isna(row[\"player_rating\"]) else row[\"player_rating\"], axis=1) # still missing so average team rating\ndf['sign'] = 2 * df['is_home'].astype(int) - 1 \ndf['rating'] = df['player_rating'] * df['sign']\nGT(df.head(10))\n\n\n\n\n\n\n\n\nseason_id\ndate\nmatch_id\nhome_team_id\naway_team_id\nold_home_elo\nold_away_elo\nnew_home_elo\nnew_away_elo\ndelta_home\ndelta_away\nplayer_team_id\nPlayer\nplayer_rating\nis_home\nsign\nrating\n\n\n\n\n68731\n2023-08-14\n3009117\n475\n459\n1877.080444\n1715.651001\n1882.5312798371797\n1710.2001651628202\n5.450835837179715\n-5.450835837179715\n475\nA. Onana\n8.2\nTrue\n1\n8.2\n\n\n68731\n2023-08-14\n3009117\n475\n459\n1877.080444\n1715.651001\n1882.5312798371797\n1710.2001651628202\n5.450835837179715\n-5.450835837179715\n475\nA. W-Bissaka\n7.6\nTrue\n1\n7.6\n\n\n68731\n2023-08-14\n3009117\n475\n459\n1877.080444\n1715.651001\n1882.5312798371797\n1710.2001651628202\n5.450835837179715\n-5.450835837179715\n475\nR. Varane\n7.4\nTrue\n1\n7.4\n\n\n68731\n2023-08-14\n3009117\n475\n459\n1877.080444\n1715.651001\n1882.5312798371797\n1710.2001651628202\n5.450835837179715\n-5.450835837179715\n475\nL. Martínez\n6.1\nTrue\n1\n6.1\n\n\n68731\n2023-08-14\n3009117\n475\n459\n1877.080444\n1715.651001\n1882.5312798371797\n1710.2001651628202\n5.450835837179715\n-5.450835837179715\n475\nL. Shaw\n7.2\nTrue\n1\n7.2\n\n\n68731\n2023-08-14\n3009117\n475\n459\n1877.080444\n1715.651001\n1882.5312798371797\n1710.2001651628202\n5.450835837179715\n-5.450835837179715\n475\nCasemiro\n7.5\nTrue\n1\n7.5\n\n\n68731\n2023-08-14\n3009117\n475\n459\n1877.080444\n1715.651001\n1882.5312798371797\n1710.2001651628202\n5.450835837179715\n-5.450835837179715\n475\nAntony\n6.5\nTrue\n1\n6.5\n\n\n68731\n2023-08-14\n3009117\n475\n459\n1877.080444\n1715.651001\n1882.5312798371797\n1710.2001651628202\n5.450835837179715\n-5.450835837179715\n475\nB. Fernandes\n6.3\nTrue\n1\n6.3\n\n\n68731\n2023-08-14\n3009117\n475\n459\n1877.080444\n1715.651001\n1882.5312798371797\n1710.2001651628202\n5.450835837179715\n-5.450835837179715\n475\nM. Mount\n5.8\nTrue\n1\n5.8\n\n\n68731\n2023-08-14\n3009117\n475\n459\n1877.080444\n1715.651001\n1882.5312798371797\n1710.2001651628202\n5.450835837179715\n-5.450835837179715\n475\nA. Garnacho\n6.2\nTrue\n1\n6.2\n\n\n\n\n\n\n\n\nFinal Step - Creating our player matrix and the \\(\\Delta\\) Elo\nCreating the covariate matrix X and the \\(\\Delta\\) Elo was done through turning the long format of the data into a wide format. Additionally coords were selected to identify the dimensions of a model.\n\n\nCode\nfin = df.copy()\nx = fin.pivot_table(index=[\"match_id\"], columns='Player', values=\"rating\").fillna(0)\nmatch_df = fin.drop_duplicates(subset='match_id').set_index('match_id')\nmatch_df = match_df.reindex(x.index) \ndelta_elo = match_df['delta_home'].values\nx_matrix = x.values\nplayer_names = x.columns.to_list()\nmatch = np.arange(x_matrix.shape[0])\ncoords = {\n    \"player\": player_names,\n    \"match\": match\n}\nprint(x_matrix)\n\n\n[[0.  5.6 0.  ... 6.5 0.  0. ]\n [0.  0.  0.  ... 0.  0.  0. ]\n [0.  0.  0.  ... 0.  0.  0. ]\n ...\n [0.  0.  0.  ... 0.  0.  0. ]\n [0.  0.  0.  ... 0.  0.  0. ]\n [0.  0.  0.  ... 0.  0.  0. ]]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tidy and Transform</span>"
    ]
  },
  {
    "objectID": "03-Vizualize.html",
    "href": "03-Vizualize.html",
    "title": "Bayesian Modelling of Elo Contribution",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\nfrom io import StringIO #yes\n#Data visualization libraries\nfrom lets_plot import *\nLetsPlot.setup_html()\nfrom great_tables import GT\nimport arviz as az\n#Web scraping libraries\nimport selenium as se\nfrom curl_cffi import requests\nfrom bs4 import BeautifulSoup\nimport time\n#For database connection and manipulation \nimport duckdb\n#For Hierarchical modeling\nimport pymc as pm\nimport xarray as xr\n#for pyshiny\nimport shiny\nimport requests as req\nimport vetiver\nimport pins\nimport logging \nimport json\nimport plotly.express as px\n\n\n\n            \n            \n            \n\n\nFor the vizualiations I picked 2 before the model and have some later that come directly from arviz library which is very useful for bayesian modelling.\nFirst we vizualize a histogram for all player ratings to see the distribution:\n\n\nCode\nplayer_ratings_df = pd.read_csv('data/player_ratings_with_elo.csv')\nmean_rating = player_ratings_df['player_rating'].mean()\nratings_hist = (\n    ggplot(player_ratings_df, aes(x='player_rating'))\n    + geom_histogram(\n        bins=70,\n        color='black',\n        fill='seagreen'\n    )\n    + geom_vline(\n        xintercept=mean_rating,\n        color=\"red\",\n        linetype=\"dashed\"\n    )\n    + labs(\n        x=\"Player Rating\",\n        y=\"Count\",\n        title=\"Distribution of Player Ratings\"\n    )\n)   \nratings_hist\n\n\n   \n   \n\n\nWe now have\n\n\nCode\ndf =pd.read_csv('data/player_ratings_with_elo.csv')\nhome_df = df[['date', 'home_team_id', 'new_home_elo']].copy()\nhome_df.rename(columns={'home_team_id': 'team', 'new_home_elo': 'elo'}, inplace=True)\naway_df = df[['date', 'away_team_id', 'new_away_elo']].copy()\naway_df.rename(columns={'away_team_id': 'team', 'new_away_elo': 'elo'}, inplace=True)\nelo_over_time_df = pd.concat([home_df, away_df], ignore_index=True)\nelo_over_time_df.sort_values(by=['team', 'date'], inplace=True)\n\n\nelo_plot = (\n    ggplot(elo_over_time_df, aes(x='date', y='elo', group='team', color='team'))\n    + geom_line()\n    + labs(\n        title=\"Team Elo Ratings Over the Season\",\n        x=\"Date\",\n        y=\"Elo Rating\"\n    ))\nelo_plot",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Vizualize</span>"
    ]
  },
  {
    "objectID": "04-Model.html",
    "href": "04-Model.html",
    "title": "5  Model",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\nfrom io import StringIO #yes\n#Data visualization libraries\nfrom lets_plot import *\nLetsPlot.setup_html()\nfrom great_tables import GT\nimport arviz as az\n#Web scraping libraries\nimport selenium as se\nfrom curl_cffi import requests\nfrom bs4 import BeautifulSoup\nimport time\n#For database connection and manipulation \nimport duckdb\n#For Hierarchical modeling\nimport pymc as pm\nimport xarray as xr\n#for pyshiny\nimport shiny\nimport requests as req\nimport vetiver\nimport pins\nimport logging \nimport json\nimport plotly.express as px\n\n\n\n            \n            \n            \n\n\nMost of my readings comprised from these links on how to model hierarchial model. The information I provide about LOO and ELPD come directly from this. https://www.pymc.io/projects/examples/en/latest/generalized_linear_models/multilevel_modeling.html https://allendowney.github.io/BayesianInferencePyMC/04_hierarchical.html https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/model_comparison.html\nso the idea is to use a Bayesian Hierarchical Model (partial pooling) that nests parameters inside parameters, and as such they are viewed as a sample from a probability distribution of parameters (taking a lot of this from pymc’s multilevel modelling module). In a completely pooled model we assume that \\(\\Delta\\) Elo.\nThis way we take into account differences in variation between our sampling units. In our context, the skill of a player realized in their ratings, can vary between each players in matches and this can handle the multicolinearity that exists between players that play with each other.\n\n\nCode\nfin = pd.read_csv('data/player_ratings_with_elo.csv')\nx = fin.pivot_table(index=[\"match_id\"], columns='Player', values=\"rating\").fillna(0)\nmatch_df = fin.drop_duplicates(subset='match_id').set_index('match_id')\nmatch_df = match_df.reindex(x.index) \ndelta_elo = match_df['delta_home'].values\nx_matrix = x.values\nplayer_names = x.columns.to_list()\nmatch = np.arange(x_matrix.shape[0])\nRANDOM_SEED = 2222\naz.style.use(\"arviz-darkgrid\")\ncoords = {\n    \"player\": player_names,\n    \"match\": match\n}\n\nwith pm.Model(coords=coords) as model_small_sigma:\n    mu_beta = pm.Normal('mu_beta', mu=0, sigma=0.5)\n    sigma_beta = pm.HalfNormal('sigma_beta', sigma=0.5)\n    beta_offset = pm.Normal('beta_offset', mu=0, sigma=1, dims=\"player\")\n    beta = pm.Deterministic('beta', mu_beta + beta_offset * sigma_beta, dims=\"player\")\n    mu = pm.Deterministic('mu', pm.math.dot(x_matrix, beta), dims=\"match\")\n    sigma = pm.HalfNormal('sigma', sigma=1)\n    y_obs = pm.Normal('y_obs', mu=mu, sigma=sigma, observed=delta_elo, dims=\"match\") \n    pp1 = pm.sample_prior_predictive(random_seed=RANDOM_SEED, samples=500)\n    trace1 = pm.sample(2500,tune=2500,random_seed=RANDOM_SEED)\n    posterior_pred1 = pm.sample_posterior_predictive(trace1, var_names=[\"y_obs\", \"mu\"], random_seed=RANDOM_SEED, return_inferencedata=True)\n    pm.compute_log_likelihood(trace1)\n    \n    \nwith pm.Model(coords=coords) as half_normal_prior:\n    mu_beta = pm.Normal('mu_beta', mu=0, sigma=0.5)\n    sigma_beta = pm.HalfNormal('sigma_beta', sigma=1)\n    beta_offset = pm.Normal('beta_offset', mu=0, sigma=1, dims=\"player\")\n    beta = pm.Deterministic('beta', mu_beta + beta_offset * sigma_beta, dims=\"player\")\n    mu = pm.Deterministic('mu', pm.math.dot(x_matrix, beta), dims=\"match\")\n    sigma = pm.HalfNormal('sigma', sigma=1)\n    y_obs = pm.Normal('y_obs', mu=mu, sigma=sigma, observed=delta_elo, dims=\"match\") \n    pp4 = pm.sample_prior_predictive(random_seed=RANDOM_SEED, samples=500)\n    trace4 = pm.sample(2500,tune=2500,random_seed=RANDOM_SEED)\n    posterior_pred4 = pm.sample_posterior_predictive(trace4, var_names=[\"y_obs\", \"mu\"], random_seed=RANDOM_SEED, return_inferencedata=True)\n    pm.compute_log_likelihood(trace4)\n\ndf_prior_pred = pd.DataFrame({\n    'delta_elo4': pp4.prior_predictive[\"y_obs\"].values.flatten(),\n    'delta_elo1': pp1.prior_predictive[\"y_obs\"].values.flatten(),\n})\n\ndf_melted = df_prior_pred.melt(var_name='Prior', value_name='delta_elo')\n(\n    ggplot(df_melted, aes(x='delta_elo', fill='Prior'))\n    + geom_histogram(\n        bins=80, \n        color=\"white\", \n        alpha=0.5,  \n        position='identity')+ labs(\n        x=\"Simulated Delta Elo\",\n        y=\"Count\",\n        title=\"Prior Predictive Distributions Comparison\"\n    )\n)\naz.plot_ppc(posterior_pred1, var_names=[\"y_obs\"])\naz.plot_ppc(posterior_pred4, var_names=[\"y_obs\"])\n\naz.plot_trace(trace1, divergences='top')\npm.model_to_graphviz(model_small_sigma)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the prior predictive checks on delta elo it seems that models with a HalfCauchy distribution are simulating huge delta elo values (in the thousands) so I’m leaving those out as that should not be possible. So I’m comparing models 1 and 4, only in terms of their sigma_beta values which are 1 and 0.5. These are relatively weak informative priors.\nI’m using Leave-one-out Cross-validation (Loo) to compare the models with different hyperparameter values. LOO cross-validation is an estimate of the out-of-sample predictive fit. Generally we select the one with the highest ELPD (Expected Log Pointwise Predictive density) is better however there’s always uncertainty around the estimates.\n\n\nCode\nmodel_small_sigma = az.loo(trace1)\nhalf_normal_prior = az.loo(trace4)\ndf_compare = az.compare({\"model_small_sigma\": model_small_sigma,\n                        \"half_normal_prior\": half_normal_prior})\ndf_compare\naz.plot_compare(df_compare, insample_dev=False)\n\n\n\n\n\n\n\n\n\nAfter running the model we use the posterior_beta to create a player contribution metric\nWe take the posterior_beta from the model and find the prercentage contribution to elo. We create a dataset for each season for a players contribution. Here is how the final dataset looks like.\n\n\nCode\nposterior_beta = trace1.posterior[\"beta\"].mean(dim=[\"chain\", \"draw\"]).values  # shape: (n_players,)\ntotal_effect = np.sum(np.abs(posterior_beta))  # Total absolute impact\nplayer_contributions =  (np.abs(posterior_beta) / total_effect)*100  # Percentage contribution\n\ncontrib_df = pd.DataFrame({\n    \"player_name\": player_names,\n    \"contribution_pct\": player_contributions,\n    \"beta_mean\": posterior_beta \n}).sort_values(\"contribution_pct\", ascending=False)\nconn = duckdb.connect()\nconn.register('contribution', contrib_df)\nconn.register('fin', fin)\n\nquery = \"\"\"\nSELECT \n    a.*,\n    b.contribution_pct,\n    b.beta_mean\nFROM fin a\nINNER JOIN contribution b\n    ON a.Player = b.player_name\nWHERE a.season_id == 68731\n\"\"\"\ndf = conn.execute(query).df()\ndf.to_csv('data/Elo_Contributions_2023.csv', index=False)\nconn.close()\nconn = duckdb.connect()\nconn.register('contribution', contrib_df)\nconn.register('fin', fin)\n\nquery = \"\"\"\nSELECT \n    a.*,\n    b.contribution_pct,\n    b.beta_mean\nFROM fin a\nINNER JOIN contribution b\nON a.Player = b.player_name\nwhere season_id = 72764\n\"\"\"\ndf = conn.execute(query).df()\ndf.to_csv('data/Elo_Contributions_2024.csv')\nconn.close()\nprem_2023 = pd.read_csv('data/Elo_Contributions_2023.csv')\nprem_2024 = pd.read_csv('data/Elo_Contributions_2024.csv')\n\nprem_2023 = df.groupby(['season_id','date','match_id','player_team_id', 'Player']).agg({\n    'player_rating': 'mean',\n    'contribution_pct': 'mean',\n    'beta_mean': 'mean'\n}).reset_index()\nprem_2023\n\nprem_2024 = prem_2024.groupby(['season_id','date','match_id','player_team_id', 'Player']).agg({\n    'player_rating': 'mean',\n    'contribution_pct': 'mean',\n    'beta_mean': 'mean'\n}).reset_index()\nprem_2024\n\ncombined = pd.concat([prem_2023, prem_2024], ignore_index=True)\ncombined.to_csv('data/Elo_Contributions_combined.csv')\ncombined = pd.read_csv('data/Elo_Contributions_combined.csv')\ncombined = GT(combined.head(5))",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model</span>"
    ]
  },
  {
    "objectID": "05-Communicate.html",
    "href": "05-Communicate.html",
    "title": "6  Communicate",
    "section": "",
    "text": "This report will be published on github actions\nFor the Web app here’s the workflow:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Communicate</span>"
    ]
  }
]